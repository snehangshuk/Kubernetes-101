## Preface
<hr>
In the last 3 years, docker and linux containers have become the sensation of the IT industry. The promise of eliminating environment inconsistencies by packaging your software together with all its dependencies (files, libraries, environment variables, runtimes, etc) has driven Linux containers to emerge as a de-facto standard.

Once your application is wrapped in a container, you don’t need to lose time configuring a new environment. Running another "instance" of your application is easy and consumes less computational resources than typical Virtual Machines.

Even though containers consume fewer computational resources, a single machine running a dozen containers is not an ideal scenario for a professional "production environment" --- running many containers on a single machine can lead to several issues.

Just to mention a few:

The quantity of active containers are limited by the "size" of your hardware

Unlike a "cloud environment" you can’t move the workloads to where you want

The one machine becomes the SPOF (Single Point of Failure) of your application.

To achieve a serious "production environment", your Linux containers should be capable of running on more than one single machine, supporting your application’s horizontal scaling. In this scenario, features like self-healing, autoscaling, load-balancing, service-discovery, and distributed storage volumes completes the professional touch of your environment. It sounds complicated, but fortunately you can get all of these features from a single technology - Kubernetes.